---
title: "Statistic Basics and Linear Regression"
author: "Erick Jones"
date: 2020-01-01
categories: ["ORIE Basics"]
tags: ["R Markdown", "Regression", "Statistics", "Statistical Testing"]
---

## Background

This post explores some of the basic concepts of statistics. 
I mostly explore these concepts using linear regression.
This is a reproducible example if you have R Studio just  make sure you have installed the correct packages.



```{r setup, include=FALSE}
knitr::opts_chunk$set(warnings = FALSE, message = FALSE)


```


```{r}
library(DataExplorer)
library(tidyverse)
library(dplyr)
library(caret)
library(lmtest)

set.seed(1234)
```


## Datasets and Sources

http://r-statistics.co/Linear-Regression.html
https://www.statmethods.net/stats/regression.html
http://r-statistics.co/Statistical-Tests-in-R.html
http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/

Dr. Sager Utexas datasets

```{r}



data <- read.table('AustinApartmentRents1.txt', header = TRUE)
summary(data)
cor(data$Rent, data$Area)
model <- lm(Rent ~ Area, data = data)

summary(model)
test.Areas <- data.frame(Area = c (500,1000))
predict(model, newdata = test.Areas)

data1 <- read.table('AustinApartmentRents2.txt', header = TRUE)


```


## Data Explorer

A convient tool to see a lot of the initial data exploration
https://towardsdatascience.com/simple-fast-exploratory-data-analysis-in-r-with-dataexplorer-package-e055348d9619

```{r}
plot_str(data1)
plot_missing(data1)
plot_histogram(data1)
plot_density(data1)
plot_correlation(data1)
plot_bar(data1)
#create_report(data1) #This creates an HTML report of all the above information and more
```


## Confidence Intervals

```{r}
#Confidence intervals around indivudual values
pred.int <- predict(model, interval = 'prediction')
#Confidence intervals around means
pred.conf <- predict(model, interval = 'confidence')

cbind(data,pred.int,pred.conf)
```

## Regression line + confidence intervals

```{r}
mydata <- cbind(data, pred.int)

p <- ggplot(mydata, aes(Area, Rent)) +
  geom_point() +
  stat_smooth(method = lm)

p
```

# Prediction intervals

```{r}

p + geom_line(aes(y = lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y = upr), color = "red", linetype = "dashed")
```


## T Test for samples

```{r}

sample1 <- sample_n(data,40)

model1 <- lm(Rent ~ Area, data = data)

p1 <- predict(model1, interval = 'confidence', level = 0.95)

summary(p1)

t.test(p1, mu = 550)
```

## MultiVariable Linear Regression

```{r}



summary(data1)

model2 <- lm(Rent ~ Area + Bathrooms, data = data1)
summary(model2)

test.bathrooms <- data.frame(Area = c(500,1000), Bathrooms = c(1,2))

p2a <- predict(model2, newdata = test.bathrooms, interval =  'confidence')


p2 <- as.data.frame(predict(model2, interval = 'confidence', level = 0.95))

cbind(p2, data1)

p2a

```
```{r}
model3 <- lm(Rent ~ Area + Bathrooms + Security + Parking + Distance, data = data1)
summary(model3)

model4 <- lm(Rent ~ Distance + Parking + Security + Bathrooms + Area, data = data1)
summary(model4)
```


## Multicollinearity
http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/


```{r}

y <- runif(50,min=0, max =100)
x1 <- runif(50, min = 0, max = 100)
x2 <- runif(50, min = 0, max = 100)
z1 <- x1+x2
z2 <- x1 + x2 + 0.005*runif(50,min=0, max =100)



list <- cbind(y,x1,x2,z1,z2)
list

cor(list)

#model catches exact multicollinearity easily
mcmodel1 <- lm(y ~ x1+x2+z1+z2)
summary(mcmodel1)

#has a harder time catching near multicollinearity useful to use VIF or tolerance
mcmodel2 <- lm(y ~ x1+x2+z2)
summary(mcmodel2)
car::vif(mcmodel2)

#Can drop the collinear term
mcmodel3 <- lm(y ~ x1+x2)
summary(mcmodel3)
car::vif(mcmodel3)

#Or drop the other one
mcmodel4 <- lm(y ~ x1+z2)
summary(mcmodel4)
car::vif(mcmodel4)


```


## Normality check
https://www.statmethods.net/stats/regression.html


```{r}



nmodel <- summary(model2)
nmodel$residuals

layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(model2)
hist(nmodel$residuals)

#Runs different tests for normality run from the predicted values for rent

p3 <- predict(model2)
summary(p3)
sd(p3)
#H0: from normal distribution p < 0.05 reject
shapiro.test(p3)
#compares if two samples are from same distribution so comparing to a normal distribution H0: from different distributions p < 0.05 reject
ks.test(p3, rnorm(60,572.3,125.7441))

```

## Linearity Test
https://bookdown.org/ccolonescu/RPoE4/further-inference-in-multiple-regression.html
http://r-statistics.co/Statistical-Tests-in-R.html


```{r}


#Ramsey RESET test test whether higher order polynomials are necessary H0: no higher order polynomials are necssary
resettest(model3, power = 2:3, type = 'fitted')
resettest(model3, power = 2:3, type = 'regressor')
```

## Heteroscedasticity

Put simply, heteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.
http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html


Both of these tests use log parameters as well as lag and leads to determine if the variance changes or if the predictors are truly independant of each other

```{r}


#Fisher Test can be used to tell if two samples have the same variance H0: ratio of variances is 1 aka they are the same p < 0.05 reject H0
var.test(sample(35,p3, replace = TRUE),sample(35,p3, replace = TRUE))

#Independence 

#Chi square tests if two caterogical variables are dependant on each other H0: variables are independant p < 0.05 reject H0
chisq.test(table(data1$Bedrooms,data1$Bathrooms))

summary(table(data1$Bedrooms,data1$Bathrooms))

#Correlation test between two continuous variables H0: correlation is 0 aka they are independant p < 0.05 reject H0

#All show some correlation
cor.test(data1$Rent, data1$Area)
cor.test(data1$Bedrooms, data1$Area)
cor.test(data1$Bedrooms, data1$Bathrooms)
cor.test(data1$Bedrooms, data1$Bathrooms)

#Are uncorrelated
cor.test(data1$Area, data1$Distance)



```

## Other helpful Stat and R learning
http://faculty.marshall.usc.edu/gareth-james/ISL/index.html
https://web.stanford.edu/~hastie/ElemStatLearn/

Essentials of Machine Learning https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
CLT and Stat Basics https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/


